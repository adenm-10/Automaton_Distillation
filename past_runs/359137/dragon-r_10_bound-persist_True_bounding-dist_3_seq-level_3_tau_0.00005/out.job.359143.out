entered training run...


============================================
Discrete Steps until Terminal State: 1000
Continuous Steps until Terminal State: 1000
3.7.2 | packaged by conda-forge | (default, Mar 20 2019, 01:46:52) 
[GCC 7.3.0]
/lustre/fs1/home/amckinney/Local_Neuro_Symbolic
Bounding Persist: <class 'bool'>, True
Bounding Distance: <class 'int'>, 3
Terminal Reward: <class 'int'>, 10
Sequence Level: <class 'int'>, 3
Dungeon Quest LTLF: F(dragon) & (!sword U key) & (!dragon U sword) & (!dragon U shield)
============================================


imported all dependencies, checking for cuda

==============
Cuda detected!
==============

[<discrete.lib.automaton.mine_aps.MineInventoryAP object at 0x2ae9b0c2db00>, <discrete.lib.automaton.mine_aps.MineInventoryAP object at 0x2ae9b0c2db38>, <discrete.lib.automaton.mine_aps.MineInventoryAP object at 0x2ae9b0c2db70>, <discrete.lib.automaton.mine_aps.MineInfoAutAP object at 0x2ae9b0c2dba8>]


============================================
Training Teacher / Independent DDPG Agent
Max Training Steps: 2000000
LTLF: F(dragon) & (!sword U key) & (!dragon U sword) & (!dragon U shield)
============================================


Starting training at: 2024-07-31 14:13:25
Env Config: EnvConfig(env_name='MineWorldEnv-v1', kwargs={'config': <discrete.lib.env.mineworldenv.MineWorldConfig object at 0x2ae9b0c2d978>}, wrapper_cls=<class 'discrete.lib.env.rew_every_step.RewEveryStep'>, wrapper_kwargs={'rew_per_step': -0.1})

Env Name: MineWorldEnv-v1
Observation Space: Box(16,)
Automaton Num States: 8
Action Space: Box(1,)
input shape: (16,)
Agent Name: DDPG Agent
Loading from checkpoint
input shape: (16,)
input shape: (16,)
Actor Learning Rate: 0.0001
Critic Learning Rate: 0.001
